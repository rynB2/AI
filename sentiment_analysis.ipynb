{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9oT+WH55cpHKo1rrCOp67"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_JlIsY1cqIX"
      },
      "source": [
        "import sklearn\r\n",
        "import nltk\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "from wordcloud import WordCloud, STOPWORDS\r\n",
        "from nltk import word_tokenize\r\n",
        "from nltk.util import ngrams\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "\r\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9G1T_11d9sO"
      },
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], #imports the IMBD dataset from tensorflow, code from https://www.tensorflow.org/hub/tutorials/tf2_text_classification\r\n",
        "                                  batch_size=-1, as_supervised=True)\r\n",
        "\r\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\r\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)\r\n",
        "\r\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZOUV9FphsOf",
        "outputId": "ab74683d-dbe5-48cf-ce28-72f8c237137c"
      },
      "source": [
        "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))\r\n",
        "full_dataset = np.concatenate((train_examples, test_examples), axis=0) #concatenate so that we can use full dataset for unigrams etc"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, test entries: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJLl3VjKjViv",
        "outputId": "991ffc11-31c9-4c7f-da1f-5b3bfc254594"
      },
      "source": [
        "df = pd.DataFrame(data=full_dataset, index=None, columns=None) #creates a pandas dataframe of every sentence in the dataset, then turns the dataframe from a numpy array type into a string.  \r\n",
        "\r\n",
        "df_str = df[0].str.decode(\"utf-8\")\r\n",
        "df_str.name = 'sentence' #names the dataframe\r\n",
        "df_str"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        This was an absolutely terrible movie. Don't b...\n",
              "1        I have been known to fall asleep during films,...\n",
              "2        Mann photographs the Alberta Rocky Mountains i...\n",
              "3        This is the kind of film for a snowy Sunday af...\n",
              "4        As others have mentioned, all the women that g...\n",
              "                               ...                        \n",
              "49995    Feeling Minnesota is not really a road movie, ...\n",
              "49996    This is, without doubt, one of my favourite ho...\n",
              "49997    Most predicable movie I've ever seen...extreme...\n",
              "49998    It's exactly what I expected from it. Relaxing...\n",
              "49999    They just don't make cartoons like they used t...\n",
              "Name: sentence, Length: 50000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfxmZePWqUuT"
      },
      "source": [
        "nltk.download('punkt') #for use in finding ngrams, code from practical\r\n",
        "from nltk import word_tokenize\r\n",
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ptY92_7qby5"
      },
      "source": [
        "def extract_unigram(sentences): #extracts unigrams, code from practical\r\n",
        "\r\n",
        "  tokens = []\r\n",
        "  for sentence in df_str:\r\n",
        "    tok = word_tokenize(sentence)\r\n",
        "    for t in tok:\r\n",
        "      tokens.append(t)\r\n",
        "  return tokens"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtYOZ37w13p5"
      },
      "source": [
        "def extract_bigram(sentences): #extracts bigrams, code from practical\r\n",
        "\r\n",
        "  all_bigrams = []\r\n",
        "  for sentence in df_str:\r\n",
        "    token = word_tokenize(sentence)\r\n",
        "    bigrams = ngrams(token,2)\r\n",
        "    for b in bigrams:\r\n",
        "      all_bigrams.append(b)\r\n",
        "  return all_bigrams"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kunzzhrh2Vgp"
      },
      "source": [
        "extracted_bigrams = extract_bigram(df_str)\r\n",
        "extracted_bigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE_y8_rysMxd"
      },
      "source": [
        "extracted_unigrams = extract_unigram(df_str)\r\n",
        "extracted_unigrams\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ilXNukvWGs"
      },
      "source": [
        "str_unigrams=(\" \").join(extracted_unigrams) #generates a wordcloud to show most used unigrams\r\n",
        "stopwords_set = {\"br\", \",\", \"<\", \">\", \".\", \":\"} #\"words\" to be excluded from wordcloud\r\n",
        "#stopwords = set(STOPWORDS) commented out as it excluded common words we could want\r\n",
        "wordcloud = WordCloud(width = 800, height = 800, \r\n",
        "                background_color ='white', \r\n",
        "                stopwords = stopwords_set, \r\n",
        "                min_font_size = 10,).generate(str_unigrams) \r\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3iKaWtewxql"
      },
      "source": [
        "plt.figure(figsize = (8, 8), facecolor = None) \r\n",
        "plt.imshow(wordcloud) \r\n",
        "plt.axis(\"off\") \r\n",
        "plt.tight_layout(pad = 0) \r\n",
        "  \r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}